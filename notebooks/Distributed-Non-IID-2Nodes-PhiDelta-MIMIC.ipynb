{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from common.utility import to_categorical, torch_device\n",
    "from notebook_utils.generate_gaussian import generate_gaussian\n",
    "from notebook_utils.eigan import Encoder, Discriminator\n",
    "from notebook_utils.federated import federated\n",
    "from notebook_utils.eigan_training import distributed_3, centralized_3\n",
    "from notebook_utils.utility import class_plot, to_numpy\n",
    "import notebook_utils.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='gpu'\n",
    "device = torch_device(device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_ally, y_advr_1, y_advr_2 = pkl.load(\n",
    "    open('../checkpoints/mimic/processed_data_X_y_ally_y_advr_y_advr_2.pkl', 'rb'))\n",
    "\n",
    "y_ally = y_ally.reshape(-1, 1)\n",
    "y_advr_1 = y_advr_1.reshape(-1, 1)\n",
    "y_advr_2 = y_advr_2.reshape(-1, 1)\n",
    "\n",
    "\n",
    "width=0.2\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(9, 4))\n",
    "ax1.bar(np.unique(y_ally.flatten())-width, np.bincount(y_ally.flatten()), width, color='b')\n",
    "ax1.bar(np.unique(y_advr_1.flatten()), np.bincount(y_advr_1.flatten()), width, color='r', hatch='o')\n",
    "ax1.bar(np.unique(y_advr_2.flatten())+width, np.bincount(y_advr_2.flatten()), width, color='r', hatch='-')\n",
    "ax1.legend(['ally', 'adversary 1', 'adversary 2'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X.shape, y_ally.shape, y_advr_1.shape, y_advr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.permutation(X.shape[0])\n",
    "crossover = int(0.3*X.shape[0])\n",
    "\n",
    "X1, X2 = X[rand_idx[:crossover]], X[rand_idx[crossover:]] \n",
    "y_ally_1, y_ally_2 = y_ally[rand_idx[:crossover]], y_ally[rand_idx[crossover:]]\n",
    "y_advr_11, y_advr_12 = y_advr_1[rand_idx[:crossover]], y_advr_1[rand_idx[crossover:]]\n",
    "y_advr_21, y_advr_22 = y_advr_2[rand_idx[:crossover]], y_advr_2[rand_idx[crossover:]]\n",
    "\n",
    "width=0.2\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax1.bar(np.unique(y_ally_1.flatten())-width, np.bincount(y_ally_1.flatten()), width, color='b')\n",
    "ax1.bar(np.unique(y_advr_11.flatten()), np.bincount(y_advr_11.flatten()), width, color='r', hatch='o')\n",
    "ax1.bar(np.unique(y_advr_21.flatten())+width, np.bincount(y_advr_21.flatten()), width, color='r', hatch='-')\n",
    "ax1.set_title('@1')\n",
    "ax2.bar(np.unique(y_ally_2.flatten())-width, np.bincount(y_ally_2.flatten()), width, color='b')\n",
    "ax2.bar(np.unique(y_advr_12.flatten()), np.bincount(y_advr_12.flatten()), width, color='r', hatch='o')\n",
    "ax2.bar(np.unique(y_advr_22.flatten())+width, np.bincount(y_advr_22.flatten()), width, color='r', hatch='-')\n",
    "ax2.set_title('@2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 1024\n",
    "history = {}\n",
    "NUM_TRIALS = 10\n",
    "VAR1 = 4\n",
    "VAR2 = 1\n",
    "NUM_NODES = 2\n",
    "PHI=1\n",
    "BATCHSIZE=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 2\n",
    "BATCHSIZE = 512\n",
    "\n",
    "X = [X1, X2] \n",
    "y_ally = [y_ally_1, y_ally_2]\n",
    "y_advr_1 = [y_advr_11, y_advr_12]\n",
    "y_advr_2 = [y_advr_21, y_advr_22]\n",
    "\n",
    "for _ in range(NUM_NODES):\n",
    "    print('@node {}, X: {}, y_ally: {}, y_advr_1: {}, y_advr_2: {}'.format(\n",
    "        _, X[_].shape, y_ally[_].shape, y_advr_1[_].shape, y_advr_2[_].shape))\n",
    "    \n",
    "w_ally = []\n",
    "w_advr_1 = []\n",
    "w_advr_2 = []\n",
    "train_loaders = []\n",
    "X_valids = []\n",
    "X_trains = []\n",
    "y_ally_valids = []\n",
    "y_ally_trains = []\n",
    "y_advr_1_valids = []\n",
    "y_advr_1_trains = []\n",
    "y_advr_2_valids = []\n",
    "y_advr_2_trains = []\n",
    "for node_idx in range(NUM_NODES):\n",
    "    X_local = X[node_idx]\n",
    "    y_ally_local = y_ally[node_idx]\n",
    "    y_advr_1_local = y_advr_1[node_idx]\n",
    "    y_advr_2_local = y_advr_2[node_idx]\n",
    "    \n",
    "    X_train, X_valid, y_ally_train, y_ally_valid, \\\n",
    "    y_advr_1_train, y_advr_1_valid, \\\n",
    "    y_advr_2_train, y_advr_2_valid = train_test_split(\n",
    "        X_local, y_ally_local, y_advr_1_local, y_advr_2_local, test_size=0.2, stratify=pd.DataFrame(\n",
    "            np.concatenate((y_ally_local, y_advr_1_local, y_advr_2_local), axis=1)\n",
    "        ))\n",
    "    print('@node {}: X_train, X_valid, y_ally_train, y_ally_valid,'\n",
    "          'y_advr_1_train, y_advr_1_valid, y_advr_2_train, y_advr_2_valid'.format(node_idx))\n",
    "    print(X_train.shape, X_valid.shape, \n",
    "          y_ally_train.shape, y_ally_valid.shape,\n",
    "          y_advr_1_train.shape, y_advr_1_valid.shape, \n",
    "          y_advr_2_train.shape, y_advr_2_valid.shape)\n",
    "\n",
    "    w = np.bincount(y_ally_train.flatten())\n",
    "    w_ally.append(sum(w)/w)\n",
    "    w = np.bincount(y_advr_1_train.flatten())\n",
    "    w_advr_1.append(sum(w)/w)\n",
    "    w = np.bincount(y_advr_2_train.flatten())\n",
    "    w_advr_2.append(sum(w)/w)\n",
    "    print('@node {}: class weights => w_ally, w_advr_1, w_advr_2'.format(node_idx), w_ally, w_advr_1, w_advr_2)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    \n",
    "    width = 0.2\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "    ax1.bar(np.unique(\n",
    "        y_ally_train.flatten())-width, np.bincount(y_ally_train.flatten()), width, color='b')\n",
    "    ax1.bar(np.unique(\n",
    "        y_advr_1_train.flatten()), np.bincount(y_advr_1_train.flatten()), width, color='r', hatch='o')\n",
    "    ax1.bar(np.unique(\n",
    "        y_advr_2_train.flatten())+width, np.bincount(y_advr_2_train.flatten()), width, color='r', hatch='-')\n",
    "    ax1.legend(['ally', 'adversary 1', 'adversary 2'])\n",
    "    ax1.set_title('train@{}'.format(node_idx+1))\n",
    "    ax2.bar(np.unique(\n",
    "        y_ally_valid.flatten())-width, np.bincount(y_ally_valid.flatten()), width, color='b')\n",
    "    ax2.bar(np.unique(\n",
    "        y_advr_1_valid.flatten()), np.bincount(y_advr_1_valid.flatten()), width, color='r', hatch='o')\n",
    "    ax2.bar(np.unique(\n",
    "        y_advr_2_valid.flatten())+width, np.bincount(y_advr_2_valid.flatten()), width, color='r', hatch='-')\n",
    "    ax2.legend(['ally', 'adversary 1', 'adversary 2'])\n",
    "    ax2.set_title('valid@{}'.format(node_idx+1))\n",
    "    \n",
    "    y_ally_train = to_categorical(y_ally_train)\n",
    "    y_ally_valid = to_categorical(y_ally_valid)\n",
    "    y_advr_1_train = to_categorical(y_advr_1_train)\n",
    "    y_advr_2_train = to_categorical(y_advr_2_train)\n",
    "    y_advr_1_valid = to_categorical(y_advr_1_valid)\n",
    "    y_advr_2_valid = to_categorical(y_advr_2_valid)\n",
    "\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_ally_train = torch.Tensor(y_ally_train)\n",
    "    y_advr_1_train = torch.Tensor(y_advr_1_train)\n",
    "    y_advr_2_train = torch.Tensor(y_advr_2_train)\n",
    "\n",
    "    X_valids.append(torch.Tensor(X_valid))\n",
    "    y_ally_valids.append(torch.Tensor(y_ally_valid))\n",
    "    y_advr_1_valids.append(torch.Tensor(y_advr_1_valid))\n",
    "    y_advr_2_valids.append(torch.Tensor(y_advr_2_valid))\n",
    "    \n",
    "    X_trains.append(X_train)\n",
    "    y_ally_trains.append(y_ally_train)\n",
    "    y_advr_1_trains.append(y_advr_1_train)\n",
    "    y_advr_2_trains.append(y_advr_2_train)\n",
    "\n",
    "    print('@node {}: tensor sizes =>'.format(node_idx), X_train.shape, X_valid.shape, \n",
    "          y_ally_train.shape, y_ally_valid.shape,\n",
    "          y_advr_1_train.shape, y_advr_1_valid.shape, y_advr_2_train.shape, y_advr_2_valid.shape)\n",
    "\n",
    "    train_loaders.append(DataLoader(TensorDataset(X_train, y_ally_train, y_advr_1_train, y_advr_2_train), \n",
    "                                    batch_size=BATCHSIZE, shuffle=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "lr_encd = 0.0001\n",
    "lr_ally = 0.00001\n",
    "lr_advr_1 = 0.00001\n",
    "lr_advr_2 = 0.00001\n",
    "n_iter_gan = 501\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = input_size*8\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {}\n",
    "encoders = {}\n",
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pkl.load(open('mimic_var_phi_delta_1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 2\n",
    "for phi in range(0, 11, 2):\n",
    "    phi /= 10\n",
    "    print(\"-\"*80)\n",
    "    print('EIGAN Training w/ phi={} and delta={}'.format(phi, delta))\n",
    "    print(\"-\"*80)\n",
    "    encoders['{}_{}'.format(phi, delta)] = distributed_3(NUM_NODES, phi, delta, \n",
    "                   X_trains, X_valids, \n",
    "                   y_ally_train, y_ally_valids,                            \n",
    "                   y_advr_1_trains, y_advr_1_valids,\n",
    "                   y_advr_2_trains, y_advr_2_valids,\n",
    "                   input_size, hidden_size, [2, 2, 2], \n",
    "                   alpha, lr_encd, lr_ally, lr_advr_1, lr_advr_2,w_ally, w_advr_1, w_advr_2,\n",
    "                   train_loaders, n_iter_gan, device, global_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(encoders, open('encoders_mimic_num_nodes_2_phi_var_delta_2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "encoders = pkl.load(open('encoders_mimic_num_nodes_2_phi_var_delta_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.cat(X_trains, dim=0).to(device)\n",
    "X_valid = torch.cat(X_valids, dim=0).to(device)\n",
    "y_ally_train = torch.cat(y_ally_trains, dim=0).to(device)\n",
    "y_ally_valid = torch.cat(y_ally_valids, dim=0).to(device)\n",
    "y_advr_1_train = torch.cat(y_advr_1_trains, dim=0).to(device)\n",
    "y_advr_1_valid = torch.cat(y_advr_1_valids, dim=0).to(device)\n",
    "y_advr_2_train = torch.cat(y_advr_2_trains, dim=0).to(device)\n",
    "y_advr_2_valid = torch.cat(y_advr_2_valids, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train, y_ally_train, y_advr_1_train, y_advr_2_train), batch_size=BATCHSIZE, shuffle=True)\n",
    "\n",
    "encoder = centralized_3(X_train, X_valid,\n",
    "                      y_ally_train, y_ally_valid,\n",
    "                      y_advr_1_train, y_advr_1_valid,\n",
    "                      y_advr_2_train, y_advr_2_valid,\n",
    "                      input_size, hidden_size, [2, 2, 2],\n",
    "                      alpha, lr_encd, lr_ally, lr_advr_1, lr_advr_2, w_ally[0], w_advr_1[0], w_advr_2[0],\n",
    "                      train_loader, n_iter_gan, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(encoder, open('encoder_mimic_num_nodes_2_phi_delta.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "encoder = pkl.load(open('encoder_mimic_num_nodes_2_phi_delta.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print('ALLY: BASELINE')\n",
    "print(\"-\"*80)\n",
    "history['baseline_ally'] = metrics.centralized(None, \n",
    "                                             input_size, hidden_size, output_size, \n",
    "                                             X_train, X_valid, y_1_train, y_1_valid, \n",
    "                                             w_1[0], device)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print('ADVERSARY: BASELINE')\n",
    "print(\"-\"*80)\n",
    "history['baseline_advr'] = metrics.centralized(None, \n",
    "                                             input_size, hidden_size, output_size, \n",
    "                                             X_train, X_valid, y_2_train, y_2_valid, \n",
    "                                             w_2[0], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print('ALLY: CENTRALIZED')\n",
    "print(\"-\"*80)\n",
    "history['centralized_ally'] = metrics.centralized(encoder, \n",
    "                                             input_size, hidden_size, output_size, \n",
    "                                             X_train, X_valid, y_ally_train, y_ally_valid, \n",
    "                                             w_ally[0], device, ['logistic', 'mlp'])\n",
    "\n",
    "print(\"-\"*80)\n",
    "print('ADVR 1: CENTRALIZED')\n",
    "print(\"-\"*80)\n",
    "history['centralized_advr_1'] = metrics.centralized(encoder, \n",
    "                                             input_size, hidden_size, output_size, \n",
    "                                             X_train, X_valid, y_advr_1_train, y_advr_1_valid, \n",
    "                                             w_advr_1[0], device, ['logistic', 'mlp'])\n",
    "\n",
    "print(\"-\"*80)\n",
    "print('ADVR 2: CENTRALIZED')\n",
    "print(\"-\"*80)\n",
    "history['centralized_advr_2'] = metrics.centralized(encoder, \n",
    "                                             input_size, hidden_size, output_size, \n",
    "                                             X_train, X_valid, y_advr_2_train, y_advr_2_valid, \n",
    "                                             w_advr_2[0], device, ['logistic', 'mlp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 2\n",
    "X_trains = [_.to(device) for _ in X_trains]\n",
    "X_valids = [_.to(device) for _ in X_valids]\n",
    "y_ally_trains = [_.to(device) for _ in y_ally_trains]\n",
    "y_ally_valids = [_.to(device) for _ in y_ally_valids]\n",
    "y_advr_1_trains = [_.to(device) for _ in y_advr_1_trains]\n",
    "y_advr_1_valids = [_.to(device) for _ in y_advr_1_valids]\n",
    "y_advr_2_trains = [_.to(device) for _ in y_advr_2_trains]\n",
    "y_advr_2_valids = [_.to(device) for _ in y_advr_2_valids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, encd in encoders.items():\n",
    "    \n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print('ALLY: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralize_ally_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                         input_size, hidden_size, output_size, \n",
    "                                                         X_trains, X_valids, y_ally_trains, y_ally_valids, \n",
    "                                                         w_ally[0], device, ['mlp', 'logistic'])\n",
    "    print(\"-\"*80)\n",
    "    print('ADVERSARY 1: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralized_advr_1_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                                     input_size, hidden_size, output_size, \n",
    "                                                                     X_trains, X_valids, y_advr_1_trains, y_advr_1_valids, \n",
    "                                                                     w_advr_1[0], device, ['mlp', 'logistic'])\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print('ADVERSARY: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralized_advr_2_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                                     input_size, hidden_size, output_size, \n",
    "                                                                     X_trains, X_valids, y_advr_2_trains, y_advr_2_valids, \n",
    "                                                                     w_advr_2[0], device, ['mlp', 'logistic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "encoders = pkl.load(open('encoders_mimic_num_nodes_2_phi_0.8_delta_var.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, encd in encoders.items():\n",
    "    \n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print('ALLY: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralize_ally_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                         input_size, hidden_size, output_size, \n",
    "                                                         X_trains, X_valids, y_ally_trains, y_ally_valids, \n",
    "                                                         w_ally[0], device, ['mlp', 'logistic'])\n",
    "    print(\"-\"*80)\n",
    "    print('ADVERSARY 1: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralized_advr_1_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                                     input_size, hidden_size, output_size, \n",
    "                                                                     X_trains, X_valids, y_advr_1_trains, y_advr_1_valids, \n",
    "                                                                     w_advr_1[0], device, ['mlp', 'logistic'])\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print('ADVERSARY: {}'.format(key))\n",
    "    print(\"-\"*80)\n",
    "    history['decentralized_advr_2_{}'.format(key)] = metrics.distributed(encd, NUM_NODES,\n",
    "                                                                     input_size, hidden_size, output_size, \n",
    "                                                                     X_trains, X_valids, y_advr_2_trains, y_advr_2_valids, \n",
    "                                                                     w_advr_2[0], device, ['mlp', 'logistic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(history, open('mimic_var_phi_delta_1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pkl.load(open('mimic_var_phi_delta.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 2\n",
    "baseline_ally = []\n",
    "baseline_advr = []\n",
    "eigan_ally = []\n",
    "eigan_advr_1 = []\n",
    "eigan_advr_2 = []\n",
    "dist_x = []\n",
    "dist_ally = []\n",
    "dist_advr_1 = []\n",
    "dist_advr_2 = []\n",
    "\n",
    "# tmp = history['baseline_ally'][3]\n",
    "# baseline_ally.append(max(tmp['svm'], tmp['logistic'], tmp['mlp']))\n",
    "# tmp = history['baseline_advr'][3]\n",
    "# baseline_advr.append(max(tmp['logistic'], tmp['mlp']))\n",
    "tmp = history['centralized_ally'][2]\n",
    "eigan_ally.append(max(tmp['logistic'], tmp['mlp']))\n",
    "tmp = history['centralized_advr_1'][2]\n",
    "eigan_advr_1.append(max(tmp['logistic'], tmp['mlp']))\n",
    "tmp = history['centralized_advr_2'][2]\n",
    "eigan_advr_2.append(max(tmp['logistic'], tmp['mlp']))\n",
    "\n",
    "\n",
    "fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for phi in range(2, 11, 2):\n",
    "    phi /= 10\n",
    "    dist_x.append(phi)\n",
    "    tmp = history['decentralize_{}_{}_{}'.format('ally', phi, delta)][2]\n",
    "    dist_ally.append(max(tmp['logistic'], tmp['mlp']))\n",
    "    tmp = history['decentralized_{}_{}_{}'.format('advr_1', phi, delta)][2]\n",
    "    dist_advr_1.append(max(tmp['logistic'], tmp['mlp']))\n",
    "    tmp = history['decentralized_{}_{}_{}'.format('advr_2', phi, delta)][2]\n",
    "    dist_advr_2.append(max(tmp['logistic'], tmp['mlp']))\n",
    "\n",
    "ax1.hlines(y=eigan_ally[0], xmin=0.1, xmax=1.1, color='b', linestyle='dashed')\n",
    "ax1.hlines(y=eigan_advr_1[0], xmin=0.1, xmax=1.1, color='r', linestyle='dashed')\n",
    "ax1.hlines(y=eigan_advr_2[0], xmin=0.1, xmax=1.1, color='c', linestyle='dashed')\n",
    "ax1.bar(np.array(dist_x)-0.04, dist_ally, width=0.04, color='b')\n",
    "ax1.bar(np.array(dist_x), dist_advr_1, width=0.04, color='r')\n",
    "ax1.bar(np.array(dist_x)+0.04, dist_advr_2, width=0.04, color='c')\n",
    "ax1.set_xticks(dist_x)\n",
    "ax1.set_xlim(left=0.1, right=1.1)\n",
    "ax1.legend(['E-ally', 'E-advr 1', 'E-advr 2' 'D-ally', 'D-advr 1', 'D-advr 2'])\n",
    "ax1.set_xlabel('fraction of parameters shared')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_title('(b)', y=-0.3)\n",
    "ax1.grid()\n",
    "\n",
    "phi = 0.8\n",
    "dist_x = []\n",
    "dist_ally = []\n",
    "dist_advr_1 = []\n",
    "dist_advr_2 = []\n",
    "for delta in range(2, 11, 2):\n",
    "    dist_x.append(delta)\n",
    "    tmp = history['decentralize_{}_{}_{}'.format('ally', phi, delta)][2]\n",
    "    dist_ally.append(max(tmp['logistic'], tmp['mlp']))\n",
    "    tmp = history['decentralized_{}_{}_{}'.format('advr_1', phi, delta)][2]\n",
    "    dist_advr_1.append(max(tmp['logistic'], tmp['mlp']))\n",
    "    tmp = history['decentralized_{}_{}_{}'.format('advr_2', phi, delta)][2]\n",
    "    dist_advr_2.append(max(tmp['logistic'], tmp['mlp']))\n",
    "\n",
    "ax2.hlines(y=eigan_ally[0], xmin=1, xmax=11, color='b', linestyle='dashed')\n",
    "ax2.hlines(y=eigan_advr_1[0], xmin=1, xmax=11, color='r', linestyle='dashed')\n",
    "ax2.hlines(y=eigan_advr_2[0], xmin=1, xmax=11, color='c', linestyle='dashed')\n",
    "ax2.bar(np.array(dist_x)-0.4, dist_ally, width=0.4, color='b')\n",
    "ax2.bar(np.array(dist_x), dist_advr_1, width=0.4, color='r')\n",
    "ax2.bar(np.array(dist_x)+0.4, dist_advr_2, width=0.4, color='c')\n",
    "ax2.set_xticks(dist_x)\n",
    "ax2.set_xlim(left=1, right=11)\n",
    "ax2.set_xlabel('frequency of sync')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.set_title('(a)', y=-.3)\n",
    "ax2.grid()\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "plt.savefig('distributed_eigan_comparison_var_phi_delta.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(history, open('mimic_var_phi_delta.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['centralized_ally'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.setp(bp1['boxes'], color='cyan')\n",
    "plt.setp(bp1['medians'], color='black')\n",
    "bp2 = ax1.boxplot(eigan_ally, vert=1, whis=1.5, widths=0.05, \n",
    "                  positions=[0.15, 0.35, 0.55, 0.75], showfliers=False, patch_artist=True)\n",
    "plt.setp(bp2['boxes'], color='red')\n",
    "plt.setp(bp2['medians'], color='black')\n",
    "bp3 = ax1.boxplot(baseline_ally, vert=1, whis=1.5, widths=0.05, \n",
    "                  positions=[0.25, 0.45, 0.65, 0.85], showfliers=False, patch_artist=True)\n",
    "plt.setp(bp3['boxes'], color='orange')\n",
    "plt.setp(bp3['medians'], color='black')\n",
    "ax1.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], \n",
    "           ['Bertran', 'EIGAN', 'Unencoded'], loc='upper right', prop={'size':10})\n",
    "ax1.set_xlim(left=0, right=1)\n",
    "ax1.set_title('(b)', y=-0.3)\n",
    "ax1.set_xlabel('variance along ally label')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xticks([0.2, 0.4, 0.6, 0.8])\n",
    "ax1.grid()\n",
    "\n",
    "bp1 = ax2.boxplot(bertran_advr, vert=1, whis=1.5, widths=0.05, \n",
    "                  positions=[0.2, 0.4, 0.6, 0.8], showfliers=False, patch_artist=True)\n",
    "plt.setp(bp1['boxes'], color='cyan')\n",
    "plt.setp(bp1['medians'], color='black')\n",
    "bp2 = ax2.boxplot(eigan_advr, vert=1, whis=1.5, widths=0.05, \n",
    "                  positions=[0.15, 0.35, 0.55, 0.75], showfliers=False, patch_artist=True)\n",
    "plt.setp(bp2['boxes'], color='red')\n",
    "plt.setp(bp2['medians'], color='black')\n",
    "bp3 = ax2.boxplot(baseline_advr, vert=1, whis=1.5, widths=0.05, \n",
    "                  positions=[0.25, 0.45, 0.65, 0.85], showfliers=False, patch_artist=True)\n",
    "plt.setp(bp3['boxes'], color='orange')\n",
    "plt.setp(bp3['medians'], color='black')\n",
    "ax2.set_xlim(left=0, right=1)\n",
    "ax2.set_title('(a)', y=-0.3)\n",
    "ax2.set_xlabel('variance along ally label')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.set_xticks([0.2, 0.4, 0.6, 0.8])\n",
    "ax2.grid()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "plt.savefig('0.4_advr_varying_ally_comparison.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
